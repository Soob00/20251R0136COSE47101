# -*- coding: utf-8 -*-
"""dataset1_decisiontree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a38e5CjtC8luz5xeCvrBpxF8OZdecj_a
"""

!pip install xgboost

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree, _tree, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import shap
import xgboost as xgb

pd.set_option('display.max_colwidth', None)

"""##데이터 불러오기"""

df = pd.read_csv("/content/processed_routine_dataset2.csv")
df

"""##변수 설정"""

Y = df['Happiness Score'] #타겟변수
routine_vars = [
    'Exercise Level',
    'Sleep Hours',
    'Screen Time per Day (Hours)',
    'Social Interaction Score',
    'Diet Type_Junk Food',
    'Diet Type_Keto',
    'Diet Type_Vegan',
    'Diet Type_Vegetarian'
]

X = df[routine_vars]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

"""변수 중요도 사전 평가"""

simple_dt = DecisionTreeRegressor(max_depth=3)
simple_dt.fit(X_train, Y_train)

pd.Series(simple_dt.feature_importances_, index=X.columns).sort_values(ascending=False)

"""##결정트리"""

model = DecisionTreeRegressor(
    max_depth=5,              # 트리 깊이 제한
    min_samples_split=20,     # 노드 분할 최소 샘플 수
    random_state=42
)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)
mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)
print(f"MSE: {mse:.4f}")
print(f"R² Score: {r2:.4f}")

"""### 결정 트리 출력
이미지
"""

plt.figure(figsize=(20, 10))
plot_tree(model, feature_names=X.columns, filled=True, rounded=True)
plt.title("결정 트리: 루틴 + 식이 변수 기반 행복 예측")
plt.show()

"""텍스트 출력"""

tree_rules = export_text(model, feature_names=list(X.columns))
print(tree_rules)

"""결정 트리 분기 조건 + 예측값(그 조건 만족하는 평균값)"""

def get_routine_rules(model, feature_names):
    tree = model.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree.feature
    ]
    rules = []

    def recurse(node, path):
        if tree.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree.threshold[node]
            # 왼쪽 (조건을 만족하는 경우)
            recurse(tree.children_left[node], path + [f"{name} <= {threshold:.2f}"])
            # 오른쪽 (조건을 만족하지 않는 경우)
            recurse(tree.children_right[node], path + [f"{name} > {threshold:.2f}"])
        else:
            value = tree.value[node][0][0]
            rules.append({
                "루틴 조건": " + ".join(path),
                "예측 행복 점수": round(value, 3)
            })

    recurse(0, [])
    return pd.DataFrame(rules).sort_values(by="예측 행복 점수", ascending=False)

rules_df = get_routine_rules(model, X.columns)
print(rules_df.head(10))

importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x=importances.values, y=importances.index)
plt.title("Routine Variable Importance")
plt.xlabel("Feature Importance")
plt.ylabel("Routine Variable")
plt.show()

df['Predicted_Happiness'] = model.predict(X)

# 평균 기준으로 이상 집단 탐지
mean_happiness = df['Happiness Score'].mean()

# 가짜 건강 루틴: 예측은 높지만 실제는 낮음
fake_healthy = df[
    (df['Predicted_Happiness'] > mean_happiness) &
    (df['Happiness Score'] < mean_happiness)
]

# 행복 과잉 예측자: 예측은 낮지만 실제는 높음
happiness_overachiever = df[
    (df['Predicted_Happiness'] < mean_happiness) &
    (df['Happiness Score'] > mean_happiness)
]

# 이상 집단 개수
print("평균 행복 점수:", round(mean_happiness, 3))
print("가짜 건강 루틴 수:", len(fake_healthy))
print("행복 과잉 예측자 수:", len(happiness_overachiever))

"""##Random Forest 모델"""

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
rf_model.fit(X_train, Y_train)

# 예측 및 평가
Y_pred_rf = rf_model.predict(X_test)
RF_MSE=mean_squared_error(Y_test, Y_pred_rf)
RF_R=r2_score(Y_test, Y_pred_rf)
print(f"MSE: {RF_MSE:.4f}")
print(f"R² Score: {RF_R:.4f}")

# TreeExplainer 사용
explainer = shap.Explainer(rf_model)
shap_values = explainer(X_test)

# SHAP 값 평균 기준 가장 중요한 변수 상위 N개 추출
shap.summary_plot(shap_values, X, plot_type="bar")

# SHAP 값 시각화
shap.plots.beeswarm(shap_values)

"""1. Social Interaction Score (사회적 상호작용)

빨간 점들이 오른쪽 끝에 많이 분포 →
사회적 상호작용 점수가 높을수록 행복도 예측이 증가함

행복 예측에 가장 긍정적인 기여

2. Diet Type_Vegetarian

SHAP 값이 양수 쪽에 몰림 →
채식주의자일수록 행복 예측에 긍정적인 영향

다이어트 타입 중에서는 이 변수가 가장 강한 영향

3. Sleep Hours

중간 SHAP 값 분포 →
수면 시간이 많다고 무조건 행복도 예측이 높아지진 않음

일부 고수면자에서는 오히려 마이너스 효과도 있음

(과도한 수면이 우울감과 관련된 현상”일 수 있음)

4. Screen Time per Day

파란 점이 왼쪽에 몰림 →
스크린 타임이 낮을수록 행복 예측이 높음

스크린 타임이 길수록 행복도 예측에 부정적 영향
"""

tree = rf_model.estimators_[0]  # 랜덤포레스트 내 첫 번째 트리
tree_rules = export_text(tree, feature_names=list(X.columns))
print(tree_rules)

RFrules_df = get_routine_rules(tree, X.columns)
print(RFrules_df.head(10))

df['Predicted_Happiness'] = rf_model.predict(X)

# 평균 기준으로 이상 집단 탐지
mean_happiness = df['Happiness Score'].mean()

# 가짜 건강 루틴: 예측은 높지만 실제는 낮음
rf_fake_healthy = df[
    (df['Predicted_Happiness'] > mean_happiness) &
    (df['Happiness Score'] < mean_happiness)
]

# 행복 과잉 예측자: 예측은 낮지만 실제는 높음
rf_happiness_overachiever = df[
    (df['Predicted_Happiness'] < mean_happiness) &
    (df['Happiness Score'] > mean_happiness)
]

# 이상 집단 개수
print("평균 행복 점수:", round(mean_happiness, 3))
print("가짜 건강 루틴 수:", len(fake_healthy))
print("행복 과잉 예측자 수:", len(happiness_overachiever))

"""## XGBoost 모델"""

xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# 학습
xgb_model.fit(X_train, Y_train)

# 예측
Y_pred = xgb_model.predict(X_test)

# 평가
XGBoost_MSE = mean_squared_error(Y_test, Y_pred)
XGBoost_R= r2_score(Y_test, Y_pred)
print(f"MSE: {XGBoost_MSE:.4f}")
print(f"R² Score: {XGBoost_R:.4f}")

xgb.plot_importance(xgb_model, height=0.5, importance_type='gain', show_values=False)
plt.title("XGBoost Feature Importance")
plt.show()

"""Shap 사용"""

explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_test)

shap.plots.beeswarm(shap_values)

"""##모델 별 MSE , $R^2$"""

results = pd.DataFrame({
    "모델": ["Decision Tree", "Random Forest", "XGBoost"],
    "MSE": [mse, RF_MSE, XGBoost_MSE],
    "R² Score": [r2, RF_R, XGBoost_R]
})

print(results)

"""전체적으로 음 막 설명력이 좋은 건 모르겠는데 그래도 rf가 그나마 나은 듯

##추가 XGbosst 튜닝해본거..
"""

xgb_model = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# 학습
xgb_model.fit(X_train, Y_train)

# 예측
y_pred_xgb = xgb_model.predict(X_test)

# 평가
XGBoost_MSE = mean_squared_error(y_test, y_pred_xgb)
XGBoost_R = r2_score(Y_test, y_pred_xgb)

print(f"Tuned XGBoost MSE: {XGBoost_MSE:.4f}")
print(f"Tuned XGBoost R²: {XGBoost_R:.4f}")

"""GridSearchCV를 썼는데 이러면 RF보다 R-squared가 0.001커진 결과값이 나오긴 함."""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 300, 500],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

grid_model = GridSearchCV(
    estimator=xgb.XGBRegressor(random_state=42),
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=3,
    verbose=1,
    n_jobs=-1
)

grid_model.fit(X_train, Y_train)

best_model = grid_model.best_estimator_
print("Best XGBoost MSE:", mean_squared_error(Y_test, best_model.predict(X_test)))
print("Best XGBoost R²:", r2_score(Y_test, best_model.predict(X_test)))

xgb.plot_importance(best_model, height=0.5, importance_type='gain', show_values=False)
plt.title("XGBoost+ Feature Importance")
plt.show()

explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_test)

shap.plots.beeswarm(shap_values)

"""##결정 트리가 뽑은 결과"""

fake_healthy.to_csv("fake_healthy.csv", index=False)

rf_fake_healthy.to_csv("rf_fake_healthy.csv", index=False)

happiness_overachiever.to_csv("happiness_overachiever.csv", index=False)

rf_happiness_overachiever.to_csv("rf_happiness_overachiever.csv", index=False)

rules_df.to_csv("rules_df.csv", index=False)

RFrules_df.to_csv("RFrules_df.csv", index=False)

df.to_csv("decisiontree_predict.csv", index=False)